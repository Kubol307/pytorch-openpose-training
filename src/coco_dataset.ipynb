{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import VisionDataset\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "import cv2\n",
    "from copy import deepcopy\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(train=False):\n",
    "  if train:\n",
    "    transform = A.Compose([\n",
    "        A.Resize(640, 480, 3),\n",
    "        A.HorizontalFlip(p=0.3),\n",
    "        A.VerticalFlip(p=0.3),\n",
    "        A.RandomBrightnessContrast(p=0.1),\n",
    "        A.ColorJitter(p=0.1),\n",
    "        ToTensorV2()\n",
    "    ], bbox_params=A.BboxParams(format='coco'))\n",
    "  else:\n",
    "    transform = A.Compose([\n",
    "        A.Resize(640, 480, 3),\n",
    "        ToTensorV2()\n",
    "    ], bbox_params=A.BboxParams(format='coco'))\n",
    "  return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class COCODataset(VisionDataset):\n",
    "    def __init__(self, root, split='train', transforms=None, transform=None, target_transform=None):\n",
    "        super().__init__(root, transforms, transform, target_transform)\n",
    "        self.root = root\n",
    "        self.json = os.path.join(root, 'annotations/person_keypoints_val2017.json')\n",
    "\n",
    "        # Read annotations\n",
    "        self.coco = COCO(self.json)\n",
    "        self.split = split\n",
    "\n",
    "        # extract and sort ids\n",
    "        self.ids = sorted([img['id'] for img in self.coco.imgs.values()])\n",
    "\n",
    "        # Remove empty dicts\n",
    "        self.ids = [id for id in self.ids if len(self.coco.loadAnns(self.coco.getAnnIds(id))) > 0]\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def _load_image(self, id):\n",
    "        path = os.path.join(self.root, 'images/val2017', self.coco.imgs[id]['file_name'])\n",
    "        image = cv2.imread(path)\n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        id = self.ids[index]\n",
    "        image = self._load_image(id)\n",
    "        # Load anns data\n",
    "        target = self.coco.loadAnns(self.coco.getAnnIds(id))\n",
    "        target = deepcopy(target)\n",
    "\n",
    "        # This format is required by albumentations\n",
    "        bboxes = [t['bbox'] + [t['category_id']] for t in target]\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=image, bboxes=bboxes) # it calls Compose from albumentations and transforms image and bboxes\n",
    "\n",
    "        image = transformed['image']\n",
    "        bboxes = transformed['bboxes']\n",
    "\n",
    "        new_bboxes = [] # for conversion from xywh to xyxy\n",
    "        for box in bboxes:\n",
    "            x1 = box[0]\n",
    "            y1 = box[1]\n",
    "            x2 = x1 + box[2]\n",
    "            y2 = y1 + box[3]\n",
    "            new_bboxes.append([x1, y1, x2, y2])\n",
    "\n",
    "        bboxes = torch.tensor(new_bboxes, dtype=torch.float32) # conversion from list to tensor\n",
    "\n",
    "        # conversion of some of the target values to pytorch tensors\n",
    "        final_target = {}\n",
    "        final_target['boxes'] = bboxes\n",
    "        final_target['labels'] = torch.tensor([t['category_id'] for t in target], dtype=torch.int64)\n",
    "        final_target['image_id'] = torch.tensor([t['image_id'] for t in target], dtype=torch.int64)\n",
    "        # final_target['area'] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        final_target['iscrowd'] = torch.tensor([t['iscrowd'] for t in target], dtype=torch.int64)\n",
    "\n",
    "        return image.div(255), final_target\n",
    "\n",
    "    # returns number of available images\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
